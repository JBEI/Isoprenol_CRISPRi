{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f1bccf-0028-4c1a-aeb2-311728425d77",
   "metadata": {},
   "source": [
    "##### This requires the DBTL0_Top3 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8076a360-bfdf-46bb-a515-8233b5474cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "import re\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d94231-f798-42f1-826e-14240f736548",
   "metadata": {},
   "source": [
    "#### Import Top3 data of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366e01c-0d3e-4d95-a670-63fd37649570",
   "metadata": {},
   "source": [
    "Load Top3 data from each of the DBTL cycles for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed01ed3b-c668-4862-ac78-d3f44f322541",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DBTL0.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m filenames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDBTL\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m)]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load CSV files and concatenate into a single dataframe\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m filenames \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDBTL\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m7\u001b[39m)]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load CSV files and concatenate into a single dataframe\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dataframes \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m filenames]\n\u001b[0;32m      6\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(dataframes, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DBTL0.csv'"
     ]
    }
   ],
   "source": [
    "# Define the filenames and the samples of interest\n",
    "filenames = [f'DBTL{i}.csv' for i in range(7)]\n",
    "\n",
    "# Load CSV files and concatenate into a single dataframe\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d59c935-1914-4852-a4f8-f85875bce102",
   "metadata": {},
   "source": [
    "Translate Top3 proteins into locus identities and then transform all of the .csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765e21c-f755-45d5-bad0-b323e511ff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the name_df DataFrame for translation\n",
    "name_df = pd.read_csv('proteomics_id_translator_240305.csv') \n",
    "\n",
    "# Create a dictionary from the name_df for fast lookup\n",
    "translator_dict = pd.Series(name_df['locus'].values, index=name_df['extracted']).to_dict()\n",
    "\n",
    "# Use the dictionary to map the Protein.Group names to locus names\n",
    "combined_df['Protein.Group'] = combined_df['Protein.Group'].map(lambda x: translator_dict.get(x, x))\n",
    "\n",
    "# Count the number of non-translated protein groups\n",
    "nontranslated = combined_df['Protein.Group'].apply(lambda x: x not in translator_dict.values()).sum()\n",
    "print(f\"In total, N = {nontranslated}/{len(combined_df['Protein.Group'])} proteins were not translated to locus names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fcc6f2-2f9a-47a1-82cb-09e7bcdaea1b",
   "metadata": {},
   "source": [
    "### Initialize a DataFrame to store log2 and log10 values for all samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b2bb75-547c-4b95-ab62-ef0135b9a56d",
   "metadata": {},
   "source": [
    "Include samples across all cycles to be analyzed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66c53f2-2162-4202-a3ed-2c756d52690a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples_of_interest = [\n",
    "# \"PP_0812_PP_0813_PP_0815\",\n",
    "# \"PP_0812_PP_0814_PP_0815\",\n",
    "# \"PP_0813_PP_0814_PP_0815\",\n",
    "# \"PP_0813_PP_0815\",\n",
    " \"PP_0815\"]\n",
    "# \"PP_0814_PP_0815\"]\n",
    "\n",
    "# \"PP_0812_PP_0813\",\n",
    "# \"PP_0812_PP_0814\",\n",
    "# \"PP_0812_PP_0813_PP_0814\",\n",
    "# \"PP_0813_PP_0814\",\n",
    "# \"PP_0812\",\n",
    "# \"PP_0813\",\n",
    "# \"PP_0814\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db26f83-76d0-44bf-9ce2-d0febfe4ad00",
   "metadata": {},
   "source": [
    "Pairwise comparison of samples_of_interest to the controls pooled across all cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e881ec-1018-40d4-9eb4-0ccf1e5b2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty DataFrame to store log2 and log10 results\n",
    "log2_log10_results = pd.DataFrame()\n",
    "sample_control_pairs = {sample: \"Control\" for sample in samples_of_interest}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f3025-2015-4f47-a671-2204156790ad",
   "metadata": {},
   "source": [
    "Log2_gold_change set to zero so that we can isolate any proteins with significant differnces compared to the control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f39059d-4c0d-41f3-94b9-f6e79a14c5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define thresholds for filtering\n",
    "log2_fold_change_threshold = 0\n",
    "p_value_threshold = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330e104a-c18e-4739-8846-360e3e1a24a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates_to_include = ['R4', 'R5', 'R6']\n",
    "for sample, control in sample_control_pairs.items():\n",
    "    # Filter data for the current sample and its control\n",
    "    if sample in ['PP_0812', 'PP_0813']:\n",
    "        sample_data = combined_df[(combined_df['Sample'] == sample) & (combined_df['Replicate'].isin(replicates_to_include))]\n",
    "    else:\n",
    "        sample_data = combined_df[combined_df['Sample'] == sample]\n",
    "    \n",
    "    control_data = combined_df[combined_df['Sample'] == control]\n",
    "\n",
    "    # Debug: Check if data is loaded correctly\n",
    "    if sample_data.empty or control_data.empty:\n",
    "        print(f\"No data for {sample} or {control}\")\n",
    "        continue\n",
    "\n",
    "    # Compute mean %_of protein_abundance_Top3-method for each protein\n",
    "    sample_mean = sample_data.groupby('Protein.Group')['%_of protein_abundance_Top3-method'].mean().reset_index()\n",
    "    control_mean = control_data.groupby('Protein.Group')['%_of protein_abundance_Top3-method'].mean().reset_index()\n",
    "\n",
    "    # Rename columns for clarity\n",
    "    sample_mean.rename(columns={'%_of protein_abundance_Top3-method': 'sample_abundance'}, inplace=True)\n",
    "    control_mean.rename(columns={'%_of protein_abundance_Top3-method': 'control_abundance'}, inplace=True)\n",
    "\n",
    "    # Merge the data on Protein.Group\n",
    "    merged_data = pd.merge(sample_mean, control_mean, on='Protein.Group', how='outer')\n",
    "\n",
    "    # Calculate log2 fold change (fill missing values with NaN)\n",
    "    merged_data['log2_change'] = np.log2(merged_data['sample_abundance'] / merged_data['control_abundance'])\n",
    "\n",
    "    # Debug: Check if log2_change contains NaN values\n",
    "    if merged_data['log2_change'].isna().all():\n",
    "        print(f\"All log2 fold changes are NaN for {sample} vs {control}\")\n",
    "        continue\n",
    "\n",
    "    # Compute p-values (assume replicates data exists for actual p-value calculation)\n",
    "    sample_reps = sample_data[['Protein.Group', '%_of protein_abundance_Top3-method']]\n",
    "    control_reps = control_data[['Protein.Group', '%_of protein_abundance_Top3-method']]\n",
    "\n",
    "    p_values = []\n",
    "    for protein in merged_data['Protein.Group']:\n",
    "        group1 = sample_reps[sample_reps['Protein.Group'] == protein]['%_of protein_abundance_Top3-method']\n",
    "        group2 = control_reps[control_reps['Protein.Group'] == protein]['%_of protein_abundance_Top3-method']\n",
    "        if not group1.empty and not group2.empty:\n",
    "            _, p_val = ttest_ind(group1, group2, equal_var=False)\n",
    "        else:\n",
    "            p_val = np.nan\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    # Add p-values and log10 transformation\n",
    "    merged_data['p_value'] = p_values\n",
    "    merged_data['log10_p_value'] = -np.log10(merged_data['p_value'])\n",
    "\n",
    "    # Debug: Check if log10_p_value contains NaN values\n",
    "    if merged_data['log10_p_value'].isna().all():\n",
    "        print(f\"All log10 p-values are NaN for {sample} vs {control}\")\n",
    "        continue\n",
    "\n",
    "    # Filter based on plotting criteria\n",
    "    filtered_data = merged_data[\n",
    "        (merged_data['log2_change'].abs() > log2_fold_change_threshold) &\n",
    "        (merged_data['log10_p_value'] > -np.log10(p_value_threshold))\n",
    "    ]\n",
    "\n",
    "    # Store filtered results in log2_log10_results\n",
    "    if log2_log10_results.empty:\n",
    "        log2_log10_results = filtered_data[['Protein.Group', 'log2_change', 'log10_p_value']].copy()\n",
    "        log2_log10_results.rename(\n",
    "            columns={'log2_change': f'{sample}_log2_FC', 'log10_p_value': f'{sample}_log10_pval'},\n",
    "            inplace=True\n",
    "        )\n",
    "    else:\n",
    "        temp_results = filtered_data[['Protein.Group', 'log2_change', 'log10_p_value']].copy()\n",
    "        temp_results.rename(\n",
    "            columns={'log2_change': f'{sample}_log2_FC', 'log10_p_value': f'{sample}_log10_pval'},\n",
    "            inplace=True\n",
    "        )\n",
    "        log2_log10_results = pd.merge(\n",
    "            log2_log10_results,\n",
    "            temp_results,\n",
    "            left_on='Protein.Group',\n",
    "            right_on='Protein.Group',\n",
    "            how='outer'\n",
    "        )\n",
    "\n",
    "#     # Create volcano plot for the current sample\n",
    "#     fold_change = merged_data['log2_change']\n",
    "#     p_values = merged_data['log10_p_value']\n",
    "\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     plt.scatter(fold_change, p_values, s=15, alpha=0.25)\n",
    "#     plt.title(f\"{sample} vs {control}\")\n",
    "#     plt.xlabel(\"Log2(Fold Change)\")\n",
    "#     plt.ylabel(\"Log10(P-Value)\")\n",
    "#     plt.grid(False)\n",
    "#     plt.ylim(0,)\n",
    "#     plt.xlim(-10, 10)\n",
    "\n",
    "#     # Threshold lines\n",
    "#     plt.axvline(x=log2_fold_change_threshold, color='r', linestyle='--', linewidth=1)\n",
    "#     plt.axvline(x=-log2_fold_change_threshold, color='r', linestyle='--', linewidth=1)\n",
    "#     plt.axhline(y=-np.log10(p_value_threshold), color='g', linestyle='--', linewidth=1)\n",
    "\n",
    "#     # Annotate significant points; this adds arrows to each of the plots and is computationally intensive, will probably crash if threshold zero\n",
    "# #     texts = []\n",
    "# #     labels = merged_data['Protein.Group']\n",
    "# #     for i, label in enumerate(labels):\n",
    "# #         if (fold_change[i] > log2_fold_change_threshold or fold_change[i] <-log2_fold_change_threshold) and p_values[i] > -np.log10(p_value_threshold):\n",
    "# #             text = plt.text(fold_change[i], p_values[i], label, fontsize=10)\n",
    "# #             texts.append(text)\n",
    "\n",
    "# #     if texts:\n",
    "# #         adjust_text(texts, arrowprops=dict(arrowstyle='-', color='red', lw=1))\n",
    "\n",
    "#     # Show the plot\n",
    "#     # plt.show()\n",
    "\n",
    "# Save the filtered log2 and log10 values to a CSV file\n",
    "log2_log10_results.to_csv('No_Top3_Filter_log2_log10_values_by_significance.csv', index=False)\n",
    "print(\"Filtered Log2 and Log10 values saved to 'filtered_log2_log10_values_by_significance.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000dadf-f940-4918-ab26-3796f71a6dd3",
   "metadata": {},
   "source": [
    "Filtered for %_of protein_abundance_Top3-method >0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff939bac-b86b-4011-89e0-595be7e58298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames to store results\n",
    "log2_log10_results = pd.DataFrame()\n",
    "raw_abundance_results = pd.DataFrame()\n",
    "\n",
    "replicates_to_include = ['R4', 'R5', 'R6']\n",
    "for sample, control in sample_control_pairs.items():\n",
    "    # Filter data for the current sample and its control\n",
    "    if sample in ['PP_0812', 'PP_0813']:\n",
    "        sample_data = combined_df[(combined_df['Sample'] == sample) & (combined_df['Replicate'].isin(replicates_to_include))]\n",
    "    else:\n",
    "        sample_data = combined_df[combined_df['Sample'] == sample]\n",
    "    \n",
    "    control_data = combined_df[combined_df['Sample'] == control]\n",
    "\n",
    "    # Debug: Check if data is loaded correctly\n",
    "    if sample_data.empty or control_data.empty:\n",
    "        print(f\"No data for {sample} or {control}\")\n",
    "        continue\n",
    "\n",
    "    # Compute mean %_of protein_abundance_Top3-method for each protein\n",
    "    sample_mean = sample_data.groupby('Protein.Group')['%_of protein_abundance_Top3-method'].mean().reset_index()\n",
    "    control_mean = control_data.groupby('Protein.Group')['%_of protein_abundance_Top3-method'].mean().reset_index()\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    sample_mean.rename(columns={'%_of protein_abundance_Top3-method': 'sample_abundance'}, inplace=True)\n",
    "    control_mean.rename(columns={'%_of protein_abundance_Top3-method': 'control_abundance'}, inplace=True)\n",
    "\n",
    "    # Filter sample_mean to include only abundances > 0.001\n",
    "    sample_mean = sample_mean[sample_mean['sample_abundance'] > 0.01]\n",
    "\n",
    "    # Merge the data on Protein.Group\n",
    "    merged_data = pd.merge(sample_mean, control_mean, on='Protein.Group', how='outer')\n",
    "\n",
    "    # Calculate log2 fold change (fill missing values with NaN)\n",
    "    merged_data['log2_change'] = np.log2(merged_data['sample_abundance'] / merged_data['control_abundance'])\n",
    "\n",
    "    # Debug: Check if log2_change contains NaN values\n",
    "    if merged_data['log2_change'].isna().all():\n",
    "        print(f\"All log2 fold changes are NaN for {sample} vs {control}\")\n",
    "        continue\n",
    "\n",
    "    # Compute p-values (assume replicates data exists for actual p-value calculation)\n",
    "    sample_reps = sample_data[['Protein.Group', '%_of protein_abundance_Top3-method']]\n",
    "    control_reps = control_data[['Protein.Group', '%_of protein_abundance_Top3-method']]\n",
    "\n",
    "    p_values = []\n",
    "    for protein in merged_data['Protein.Group']:\n",
    "        group1 = sample_reps[sample_reps['Protein.Group'] == protein]['%_of protein_abundance_Top3-method']\n",
    "        group2 = control_reps[control_reps['Protein.Group'] == protein]['%_of protein_abundance_Top3-method']\n",
    "        if not group1.empty and not group2.empty:\n",
    "            _, p_val = ttest_ind(group1, group2, equal_var=False)\n",
    "        else:\n",
    "            p_val = np.nan\n",
    "        p_values.append(p_val)\n",
    "\n",
    "    # Add p-values and log10 transformation\n",
    "    merged_data['p_value'] = p_values\n",
    "    merged_data['log10_p_value'] = -np.log10(merged_data['p_value'])\n",
    "\n",
    "    # Debug: Check if log10_p_value contains NaN values\n",
    "    if merged_data['log10_p_value'].isna().all():\n",
    "        print(f\"All log10 p-values are NaN for {sample} vs {control}\")\n",
    "        continue\n",
    "\n",
    "    # Filter based on plotting criteria\n",
    "    filtered_data = merged_data[\n",
    "        (merged_data['log2_change'].abs() > log2_fold_change_threshold) &\n",
    "        (merged_data['log10_p_value'] > -np.log10(p_value_threshold))\n",
    "    ]\n",
    "\n",
    "    \n",
    "    # Store filtered results in log2_log10_results\n",
    "    if log2_log10_results.empty:\n",
    "        log2_log10_results = filtered_data[['Protein.Group', 'log2_change', 'log10_p_value']].copy()\n",
    "        log2_log10_results.rename(\n",
    "            columns={'log2_change': f'{sample}_log2_FC', 'log10_p_value': f'{sample}_log10_pval'},\n",
    "            inplace=True\n",
    "        )\n",
    "    else:\n",
    "        temp_results = filtered_data[['Protein.Group', 'log2_change', 'log10_p_value']].copy()\n",
    "        temp_results.rename(\n",
    "            columns={'log2_change': f'{sample}_log2_FC', 'log10_p_value': f'{sample}_log10_pval'},\n",
    "            inplace=True\n",
    "        )\n",
    "        log2_log10_results = pd.merge(\n",
    "            log2_log10_results,\n",
    "            temp_results,\n",
    "            left_on='Protein.Group',\n",
    "            right_on='Protein.Group',\n",
    "            how='outer'\n",
    "        )\n",
    "\n",
    "# Save the filtered log2 and log10 values to a CSV file\n",
    "log2_log10_results.to_csv('Test.csv', index=False)\n",
    "print(\"Filtered Log2 and Log10 values saved to 'Test.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd68e65-793b-4254-8bd0-94c3925a0a71",
   "metadata": {},
   "source": [
    "### Investigating genes that are upregulated amongst a subset of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b663e7-53d6-4bf3-8ee2-2fac9cfc7b57",
   "metadata": {},
   "source": [
    "#### TCA Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745043c6-8034-476e-b051-6010afd85c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second list of proteins\n",
    "proteins_txt = pd.read_csv('pathway-genes-TCA.txt', delimiter='\\t')  # Adjust the separator if needed\n",
    "proteins_txt = proteins_txt.drop_duplicates(subset='Gene Accession')\n",
    "second_list = proteins_txt['Gene Accession'].tolist()\n",
    "\n",
    "# Filter the data to include only the proteins that are significant for all samples of interest\n",
    "filtered_data = log2_log10_results.dropna(subset=[f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest])\n",
    "\n",
    "# Compare the filtered data against the second list of proteins\n",
    "matching_proteins = filtered_data[filtered_data['Protein.Group'].isin(second_list)]\n",
    "matching_proteins.to_csv('matching_proteins.csv', index=False)\n",
    "\n",
    "# Calculate the ratio of significantly changed genes\n",
    "included = matching_proteins.shape[0]\n",
    "total = len(second_list)\n",
    "ratio = included / total\n",
    "\n",
    "print(f\"{included} out of {total} genes in the TCA cycle were significantly changed across strains\")\n",
    "merged_data = pd.merge(matching_proteins, proteins_txt, left_on='Protein.Group', right_on='Gene Accession')\n",
    "\n",
    "# Extract relevant columns\n",
    "columns_to_include = ['Gene Accession', 'Gene name', 'Enzymatic activity'] + [f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest]\n",
    "locus_genes = merged_data[columns_to_include].drop_duplicates(subset='Gene Accession')\n",
    "\n",
    "# Print the genes by locus with additional information\n",
    "print(\"Those genes were:\")\n",
    "print(locus_genes.to_string(index=False))\n",
    "locus_genes.to_csv('locus_genes.csv', index=False)\n",
    "\n",
    "print(\"The genes were saved to 'locus_genes.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84814d0-1814-46df-806d-14d4d24e4f2f",
   "metadata": {},
   "source": [
    "#### Branched Chain AA Superpathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a261c9f9-df1e-4bb2-861b-a15ca3f5c8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second list of proteins\n",
    "proteins_txt = pd.read_csv('pathway-genes-BRANCHED-CHAIN-AA-SYN-PWY.txt', delimiter='\\t')  # Adjust the separator if needed\n",
    "proteins_txt = proteins_txt.drop_duplicates(subset='Gene Accession')\n",
    "second_list = proteins_txt['Gene Accession'].tolist()\n",
    "\n",
    "filtered_data = log2_log10_results.dropna(subset=[f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest])\n",
    "\n",
    "# Compare the filtered data against the second list of proteins\n",
    "matching_proteins = filtered_data[filtered_data['Protein.Group'].isin(second_list)]\n",
    "matching_proteins.to_csv('matching_proteins.csv', index=False)\n",
    "\n",
    "# Calculate the ratio of significantly changed genes\n",
    "included = matching_proteins.shape[0]\n",
    "total = len(second_list)\n",
    "ratio = included / total\n",
    "\n",
    "print(f\"{included} out of {total} genes in the AA superpathway were significantly changed across strains\")\n",
    "# Merge the matching proteins with the proteins_txt DataFrame to get additional information\n",
    "merged_data = pd.merge(matching_proteins, proteins_txt, left_on='Protein.Group', right_on='Gene Accession')\n",
    "\n",
    "# Extract relevant columns\n",
    "# Include columns for p-values and fold changes for the samples of interest\n",
    "columns_to_include = ['Gene Accession', 'Gene name', 'Enzymatic activity'] + [f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest]\n",
    "locus_genes = merged_data[columns_to_include].drop_duplicates(subset='Gene Accession')\n",
    "\n",
    "\n",
    "# Print the genes by locus with additional information\n",
    "print(\"Those genes were:\")\n",
    "print(locus_genes.to_string(index=False))\n",
    "\n",
    "locus_genes.to_csv('locus_genes.csv', index=False)\n",
    "\n",
    "print(\"The genes were saved to 'locus_genes.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda6918-c5dd-456c-9622-3393ae99fd82",
   "metadata": {},
   "source": [
    "#### Glycolysis Genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85589d24-f0cf-4470-b622-e373219cfcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second list of proteins\n",
    "proteins_txt = pd.read_csv('pathway-genes-PWY1G01-2.txt', delimiter='\\t')  # Adjust the separator if needed\n",
    "proteins_txt = proteins_txt.drop_duplicates(subset='Gene Accession')\n",
    "second_list = proteins_txt['Gene Accession'].tolist()\n",
    "\n",
    "filtered_data = log2_log10_results.dropna(subset=[f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest])\n",
    "matching_proteins = filtered_data[filtered_data['Protein.Group'].isin(second_list)]\n",
    "matching_proteins.to_csv('matching_proteins.csv', index=False)\n",
    "# Calculate the ratio of significantly changed genes\n",
    "included = matching_proteins.shape[0]\n",
    "total = len(second_list)\n",
    "ratio = included / total\n",
    "print(f\"{included} out of {total} genes in glycolysis were significantly changed across strains\")\n",
    "merged_data = pd.merge(matching_proteins, proteins_txt, left_on='Protein.Group', right_on='Gene Accession')\n",
    "columns_to_include = ['Gene Accession', 'Gene name', 'Enzymatic activity'] + [f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest]\n",
    "locus_genes = merged_data[columns_to_include].drop_duplicates(subset='Gene Accession')\n",
    "\n",
    "# Print the genes by locus with additional information\n",
    "print(\"Those genes were:\")\n",
    "print(locus_genes.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a37f622-8b36-49b3-8e0a-826ac0695928",
   "metadata": {},
   "source": [
    "#### Non-oxidative PP Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7062606-749d-49c4-a26c-ecb064eb7aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second list of proteins\n",
    "proteins_txt = pd.read_csv('pathway-genes-NONOXIPENT-PWY.txt', delimiter='\\t')  # Adjust the separator if needed\n",
    "proteins_txt = proteins_txt.drop_duplicates(subset='Gene Accession')\n",
    "second_list = proteins_txt['Gene Accession'].tolist()\n",
    "\n",
    "filtered_data = log2_log10_results.dropna(subset=[f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest])\n",
    "matching_proteins = filtered_data[filtered_data['Protein.Group'].isin(second_list)]\n",
    "matching_proteins.to_csv('matching_proteins.csv', index=False)\n",
    "# Calculate the ratio of significantly changed genes\n",
    "included = matching_proteins.shape[0]\n",
    "total = len(second_list)\n",
    "ratio = included / total\n",
    "\n",
    "print(f\"{included} out of {total} genes were significantly changed across strains\")\n",
    "merged_data = pd.merge(matching_proteins, proteins_txt, left_on='Protein.Group', right_on='Gene Accession')\n",
    "columns_to_include = ['Gene Accession', 'Gene name', 'Enzymatic activity'] + [f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest]\n",
    "locus_genes = merged_data[columns_to_include].drop_duplicates(subset='Gene Accession')\n",
    "\n",
    "# Print the genes by locus with additional information\n",
    "print(\"Those genes were:\")\n",
    "print(locus_genes.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2000c7-0f96-4e44-9590-6948dcb33079",
   "metadata": {},
   "source": [
    "#### Oxidative PP Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae749b64-5fd1-4a2b-9280-c5ca4337cd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second list of proteins\n",
    "proteins_txt = pd.read_csv('pathway-genes-OXIDATIVEPENT-PWY.txt', delimiter='\\t')  # Adjust the separator if needed\n",
    "proteins_txt = proteins_txt.drop_duplicates(subset='Gene Accession')\n",
    "second_list = proteins_txt['Gene Accession'].tolist()\n",
    "\n",
    "filtered_data = log2_log10_results.dropna(subset=[f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest])\n",
    "matching_proteins = filtered_data[filtered_data['Protein.Group'].isin(second_list)]\n",
    "matching_proteins.to_csv('matching_proteins.csv', index=False)\n",
    "# Calculate the ratio of significantly changed genes\n",
    "included = matching_proteins.shape[0]\n",
    "total = len(second_list)\n",
    "ratio = included / total\n",
    "\n",
    "print(f\"{included} out of {total} genes were significantly changed across strains\")\n",
    "merged_data = pd.merge(matching_proteins, proteins_txt, left_on='Protein.Group', right_on='Gene Accession')\n",
    "columns_to_include = ['Gene Accession', 'Gene name', 'Enzymatic activity'] + [f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest]\n",
    "locus_genes = merged_data[columns_to_include].drop_duplicates(subset='Gene Accession')\n",
    "\n",
    "# Print the genes by locus with additional information\n",
    "print(\"Those genes were:\")\n",
    "print(locus_genes.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df38a3f4-a418-46df-973f-cf6b383e2b49",
   "metadata": {},
   "source": [
    "#### PP Pathway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed9fb01-e04d-4c5f-99fa-22e1d77d2713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second list of proteins\n",
    "proteins_txt = pd.read_csv('pathway-genes-PENTOSE-P-PWY.txt', delimiter='\\t')  # Adjust the separator if needed\n",
    "proteins_txt = proteins_txt.drop_duplicates(subset='Gene Accession')\n",
    "second_list = proteins_txt['Gene Accession'].tolist()\n",
    "\n",
    "filtered_data = log2_log10_results.dropna(subset=[f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest])\n",
    "matching_proteins = filtered_data[filtered_data['Protein.Group'].isin(second_list)]\n",
    "matching_proteins.to_csv('matching_proteins.csv', index=False)\n",
    "# Calculate the ratio of significantly changed genes\n",
    "included = matching_proteins.shape[0]\n",
    "total = len(second_list)\n",
    "ratio = included / total\n",
    "\n",
    "print(f\"{included} out of {total} genes were significantly changed across strains\")\n",
    "merged_data = pd.merge(matching_proteins, proteins_txt, left_on='Protein.Group', right_on='Gene Accession')\n",
    "columns_to_include = ['Gene Accession', 'Gene name', 'Enzymatic activity'] + [f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest]\n",
    "locus_genes = merged_data[columns_to_include].drop_duplicates(subset='Gene Accession')\n",
    "\n",
    "# Print the genes by locus with additional information\n",
    "print(\"Those genes were:\")\n",
    "print(locus_genes.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4844bf-f0ca-4235-8c4f-b20d20d62e3f",
   "metadata": {},
   "source": [
    "#### Oxidative Phosphorylation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13dd87-b3dd-4fb1-89be-bd2951710381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second list of proteins\n",
    "proteins_txt = pd.read_csv('pathway-genes-PWY-Respiration_Pooled.txt', delimiter='\\t')  # Adjust the separator if needed\n",
    "proteins_txt2 = proteins_txt.drop_duplicates(subset='Gene Accession')\n",
    "second_list = proteins_txt2['Gene Accession'].tolist()\n",
    "\n",
    "filtered_data = log2_log10_results.dropna(subset=[f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest])\n",
    "matching_proteins = filtered_data[filtered_data['Protein.Group'].isin(second_list)]\n",
    "matching_proteins.to_csv('matching_proteins.csv', index=False)\n",
    "# Calculate the ratio of significantly changed genes\n",
    "included = matching_proteins.shape[0]\n",
    "total = len(second_list)\n",
    "ratio = included / total\n",
    "\n",
    "print(f\"{included} out of {total} genes were significantly changed across strains\")\n",
    "merged_data = pd.merge(matching_proteins, proteins_txt, left_on='Protein.Group', right_on='Gene Accession')\n",
    "columns_to_include = ['Gene Accession', 'Gene name', 'Enzymatic activity'] + [f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest]\n",
    "locus_genes = merged_data[columns_to_include].drop_duplicates(subset='Gene Accession')\n",
    "\n",
    "# Print the genes by locus with additional information\n",
    "print(\"Those genes were:\")\n",
    "print(locus_genes.to_string(index=False))\n",
    "locus_genes.to_csv('locus_genes.csv', index=False)\n",
    "\n",
    "print(\"The genes were saved to 'locus_genes.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8177627-4fa1-4f36-b3de-3496e5258903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the second list of proteins\n",
    "proteins_txt = pd.read_csv('glutamine_glutamate.txt', delimiter='\\t')  # Adjust the separator if needed\n",
    "proteins_txt2 = proteins_txt.drop_duplicates(subset='Gene Accession')\n",
    "second_list = proteins_txt2['Gene Accession'].tolist()\n",
    "\n",
    "filtered_data = log2_log10_results.dropna(subset=[f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest])\n",
    "matching_proteins = filtered_data[filtered_data['Protein.Group'].isin(second_list)]\n",
    "matching_proteins.to_csv('matching_proteins.csv', index=False)\n",
    "# Calculate the ratio of significantly changed genes\n",
    "included = matching_proteins.shape[0]\n",
    "total = len(second_list)\n",
    "ratio = included / total\n",
    "\n",
    "print(f\"{included} out of {total} genes were significantly changed across strains\")\n",
    "merged_data = pd.merge(matching_proteins, proteins_txt, left_on='Protein.Group', right_on='Gene Accession')\n",
    "columns_to_include = ['Gene Accession', 'Gene name', 'Enzymatic activity'] + [f\"{sample}_log2_FC\" for sample in samples_of_interest] + [f\"{sample}_log10_pval\" for sample in samples_of_interest]\n",
    "locus_genes = merged_data[columns_to_include].drop_duplicates(subset='Gene Accession')\n",
    "\n",
    "# Print the genes by locus with additional information\n",
    "print(\"Those genes were:\")\n",
    "print(locus_genes.to_string(index=False))\n",
    "locus_genes.to_csv('locus_genes.csv', index=False)\n",
    "\n",
    "print(\"The genes were saved to 'locus_genes.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046bb54-3f39-4dec-9dbc-1997d5e2ba55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
